{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "print('Installing dependencies')\n",
    "!pip install -q accelerate==0.12.0\n",
    "!pip uninstall -y diffusers\n",
    "!git clone --branch updt https://github.com/TheLastBen/diffusers\n",
    "!pip install -q ./diffuser\n",
    "\n",
    "!wget -O refmdlz https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/refmdlz\n",
    "!unzip -o -q refmdlz    \n",
    "!wget https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv1.py\n",
    "!wget https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/convertodiffv2.py\n",
    "!wget https://github.com/TheLastBen/fast-stable-diffusion/raw/main/Dreambooth/convertosd.py\n",
    "!wget https://raw.githubusercontent.com/TheLastBen/fast-stable-diffusion/main/Dreambooth/smart_crop.py\n",
    "print('Done, proceed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_Path = \"/home/zhuole/data_ssd/stable-diffusion-webui/models/Stable-diffusion/v1-5-pruned-emaonly.ckpt\"\n",
    "Model_Version = \"1.5\"\n",
    "Session_Name = \"testdb2\"\n",
    "\n",
    "IMAGES_FOLDER_OPTIONAL = \"/home/zhuole/dreambooth/datasets/person\"\n",
    "TOKEN = 'testdb2'\n",
    "Remove_existing_instance_images = True\n",
    "Crop_images = False\n",
    "Crop_size = \"512\"\n",
    "Add_background = False\n",
    "Upscale_image = True\n",
    "\n",
    "Resume_Training = False\n",
    "UNet_Training_Steps = 1500\n",
    "UNet_Learning_Rate = 2e-5\n",
    "Text_Encoder_Training_Steps = 150\n",
    "Text_Encoder_Concept_Training_Steps = 0\n",
    "Text_Encoder_Learning_Rate = 1e-6\n",
    "Style_Training = True\n",
    "Resolution = \"512\"\n",
    "\n",
    "Save_Checkpoint_Every_n_Steps = True\n",
    "Save_Checkpoint_Every = 500\n",
    "Start_saving_from_the_step = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "if CKPT_Path != \"\":\n",
    "  clear_output()\n",
    "  with capture.capture_output() as cap:\n",
    "    if Model_Version=='1.5':\n",
    "      !python convertodiffv1.py \"$CKPT_Path\" stable-diffusion-custom --v1\n",
    "    elif Model_Version=='V2.1-512px':\n",
    "      !python convertodiffv2.py \"$CKPT_Path\" stable-diffusion-custom --v2 --reference_model stabilityai/stable-diffusion-2-1-base\n",
    "    elif Model_Version=='V2.1-768px':\n",
    "      !python convertodiffv2.py \"$CKPT_Path\" stable-diffusion-custom --v2 --reference_model stabilityai/stable-diffusion-2-1\n",
    "  if os.path.exists('stable-diffusion-custom/unet/diffusion_pytorch_model.bin'):\n",
    "    MODEL_NAME=\"stable-diffusion-custom\"\n",
    "    print('DONE !')\n",
    "  else:\n",
    "    !rm -r stable-diffusion-custom\n",
    "    print('Conversion error') \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading session with no previous model, using the original model or the custom downloaded model\n",
      "Session Loaded, proceed to uploading instance images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "from IPython.utils import capture\n",
    "from os import listdir\n",
    "from os.path import isfile\n",
    "import time\n",
    "\n",
    "PT=\"\"\n",
    "WORKSPACE='.'\n",
    "INSTANCE_NAME=Session_Name\n",
    "OUTPUT_DIR=WORKSPACE+'/'+Session_Name\n",
    "SESSION_DIR=WORKSPACE+'/Sessions/'+Session_Name\n",
    "INSTANCE_DIR=SESSION_DIR+'/instance_images'\n",
    "CONCEPT_DIR=SESSION_DIR+'/concept_images'\n",
    "MDLPTH=str(SESSION_DIR+\"/\"+Session_Name+'.ckpt')\n",
    "\n",
    "\n",
    "if os.path.exists(str(SESSION_DIR)):\n",
    "  mdls = [ckpt for ckpt in listdir(SESSION_DIR) if ckpt.split(\".\")[-1 ]== \"ckpt\"]\n",
    "  if not os.path.exists(MDLPTH) and '.ckpt' in str(mdls):  \n",
    "\n",
    "    def f(n):  \n",
    "      k=0\n",
    "      for i in mdls:    \n",
    "        if k==n:    \n",
    "          !mv \"$SESSION_DIR/$i\" $MDLPTH\n",
    "        k=k+1\n",
    "\n",
    "    k=0\n",
    "    print('No final checkpoint model found, select which intermediary checkpoint to use, enter only the number, (000 to skip):\\n')\n",
    "\n",
    "    for i in mdls:    \n",
    "      print(str(k)+'- '+i)\n",
    "      k=k+1\n",
    "    n=input()\n",
    "    while int(n)>k-1:\n",
    "      n=input()  \n",
    "    if n!=\"000\":\n",
    "      f(int(n))\n",
    "      print('Using the model '+ mdls[int(n)]+\" ...\")\n",
    "      time.sleep(2)\n",
    "    else:\n",
    "      print('Skipping the intermediary checkpoints.')\n",
    "    del n\n",
    "\n",
    "if os.path.exists(str(SESSION_DIR)) and not os.path.exists(MDLPTH):\n",
    "  print('Loading session with no previous model, using the original model or the custom downloaded model')\n",
    "  if MODEL_NAME==\"\":\n",
    "    print('No model found, please specify a model.')\n",
    "  else:\n",
    "    print('Session Loaded, proceed to uploading instance images')\n",
    "\n",
    "elif not os.path.exists(str(SESSION_DIR)):\n",
    "    %mkdir -p \"$INSTANCE_DIR\"\n",
    "    print('Creating session...')\n",
    "    if MODEL_NAME==\"\":\n",
    "      print('No model found, please specify a model.')\n",
    "    else:\n",
    "      print('Session created, proceed to uploading instance images')\n",
    "      \n",
    "elif os.path.exists(MDLPTH):\n",
    "  print('Session found, loading the trained model ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  |███████████████| 19/19 Uploaded"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done, proceed to the next cell\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from smart_crop import *\n",
    "\n",
    "if Remove_existing_instance_images:\n",
    "  if os.path.exists(str(INSTANCE_DIR)):\n",
    "    !rm -r \"$INSTANCE_DIR\"\n",
    "\n",
    "if not os.path.exists(str(INSTANCE_DIR)):\n",
    "  %mkdir -p \"$INSTANCE_DIR\"\n",
    "\n",
    "Crop_size=int(Crop_size)\n",
    "\n",
    "if IMAGES_FOLDER_OPTIONAL!=\"\":\n",
    "  if Crop_images:\n",
    "    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n",
    "      extension = filename.split(\".\")[-1]\n",
    "      identifier=filename.split(\".\")[0]\n",
    "      new_path_with_file = os.path.join(INSTANCE_DIR, filename)\n",
    "      file = Image.open(IMAGES_FOLDER_OPTIONAL+\"/\"+filename)\n",
    "      width, height = file.size\n",
    "      if file.size !=(Crop_size, Crop_size):  \n",
    "        image = crop_image(file, Crop_size)    \n",
    "        # side_length = min(width, height)\n",
    "        # left = (width - side_length)/2\n",
    "        # top = (height - side_length)/2\n",
    "        # right = (width + side_length)/2\n",
    "        # bottom = (height + side_length)/2\n",
    "        # image = file.crop((left, top, right, bottom))\n",
    "        # image = image.resize((Crop_size, Crop_size))\n",
    "        if (extension.upper() == \"JPG\"):\n",
    "            image[0].save(new_path_with_file, format=\"JPEG\", quality = 100)\n",
    "        else:\n",
    "            image[0].save(new_path_with_file, format=extension.upper())\n",
    "      else:\n",
    "        !cp \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n",
    "\n",
    "  else:\n",
    "    for filename in tqdm(os.listdir(IMAGES_FOLDER_OPTIONAL), bar_format='  |{bar:15}| {n_fmt}/{total_fmt} Uploaded'):\n",
    "      %cp -r \"$IMAGES_FOLDER_OPTIONAL/$filename\" \"$INSTANCE_DIR\"\n",
    " \n",
    "  print('Done, proceed to the next cell')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "files = glob.glob(os.path.join(INSTANCE_DIR, '*'))\n",
    "os.makedirs(INSTANCE_DIR+\"_tmp\", exist_ok=True)\n",
    "for idx, i in enumerate(files):\n",
    "  if Add_background:\n",
    "    im = Image.open(i).convert(\"RGBA\")\n",
    "    new_image = Image.new(\"RGBA\", im.size, \"WHITE\")\n",
    "    new_image.paste(im, (0, 0), im)\n",
    "    new_image.convert('RGB').save(os.path.join(INSTANCE_DIR+\"_tmp\", '%s(%d).jpg'%(TOKEN, idx+1)), quality=100)\n",
    "  else:\n",
    "    im = Image.open(i)\n",
    "    im.convert('RGB').save(os.path.join(INSTANCE_DIR+\"_tmp\", '%s(%d).jpg'%(TOKEN, idx+1)), quality=100)\n",
    "\n",
    "shutil.rmtree(INSTANCE_DIR)\n",
    "os.rename(INSTANCE_DIR+'_tmp', INSTANCE_DIR)\n",
    "\n",
    "if Upscale_image:\n",
    "  INSTANCE_DIR_NEW = INSTANCE_DIR+'_upscaled'\n",
    "  with capture.capture_output() as cap:\n",
    "    !python ./Real-ESRGAN/inference_realesrgan.py -n RealESRGAN_x4plus_anime_6B -i \"$INSTANCE_DIR\" -o \"$INSTANCE_DIR_NEW\"\n",
    "  files = glob.glob(os.path.join(INSTANCE_DIR_NEW, '*'))\n",
    "  os.makedirs(INSTANCE_DIR+\"_tmp\", exist_ok=True)\n",
    "  for idx, i in enumerate(files):\n",
    "    im = Image.open(i)\n",
    "    im.convert('RGB').save(os.path.join(INSTANCE_DIR+\"_tmp\", '%s(%d).jpg'%(TOKEN, idx+1)), quality=100)\n",
    "  shutil.rmtree(INSTANCE_DIR_NEW)\n",
    "  shutil.rmtree(INSTANCE_DIR)\n",
    "  os.rename(INSTANCE_DIR+'_tmp', INSTANCE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the UNet...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--num_processes` was set to a value of `1`\n",
      "\t`--num_machines` was set to a value of `1`\n",
      "\t`--mixed_precision` was set to a value of `'no'`\n",
      "\t`--num_cpu_threads_per_process` was set to `14` to improve out-of-box performance\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "/opt/anaconda/anaconda3/envs/ldm/lib/python3.8/site-packages/diffusers/utils/deprecation_utils.py:35: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  warnings.warn(warning + message, FutureWarning)\n",
      "Progress:|                         |:   1%| | 16/1500 [00:06<09:00,  2.75it/s, l \u001b[0;32m \u001b[0m \u001b[0;32m \u001b[0m^C\n",
      "Traceback (most recent call last):\n",
      "  File \"./diffusers/examples/dreambooth/train_dreambooth.py\", line 852, in <module>\n",
      "    main()\n",
      "  File \"./diffusers/examples/dreambooth/train_dreambooth.py\", line 726, in main\n",
      "    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n",
      "  File \"/home/zhuole/.local/lib/python3.8/site-packages/accelerate/accelerator.py\", line 921, in clip_grad_norm_\n",
      "    torch.nn.utils.clip_grad_norm_(parameters, max_norm, norm_type=norm_type)\n",
      "  File \"/opt/anaconda/anaconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py\", line 43, in clip_grad_norm_\n",
      "    total_norm = torch.norm(torch.stack([torch.norm(g.detach(), norm_type).to(device) for g in grads]), norm_type)\n",
      "  File \"/opt/anaconda/anaconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py\", line 43, in <listcomp>\n",
      "    total_norm = torch.norm(torch.stack([torch.norm(g.detach(), norm_type).to(device) for g in grads]), norm_type)\n",
      "KeyboardInterrupt\n",
      "Progress:|                         |:   1%| | 16/1500 [00:07<11:11,  2.21it/s, l\n",
      "Something went wrong\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from subprocess import getoutput\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import random\n",
    "\n",
    "if os.path.exists(INSTANCE_DIR+\"/.ipynb_checkpoints\"):\n",
    "  %rm -r $INSTANCE_DIR\"/.ipynb_checkpoints\"\n",
    "\n",
    "if os.path.exists(CONCEPT_DIR+\"/.ipynb_checkpoints\"):\n",
    "  %rm -r $CONCEPT_DIR\"/.ipynb_checkpoints\"  \n",
    "\n",
    "try:\n",
    "   resume\n",
    "   if resume and not Resume_Training:\n",
    "     print('Overwrite your previously trained model ?, answering \"yes\" will train a new model, answering \"no\" will resume the training of the previous model?  yes or no ?')\n",
    "     while True:\n",
    "        ansres=input('')\n",
    "        if ansres=='no':\n",
    "          Resume_Training = True\n",
    "          del ansres\n",
    "          break\n",
    "        elif ansres=='yes':\n",
    "          Resume_Training = False\n",
    "          resume= False\n",
    "          break\n",
    "except:\n",
    "  pass\n",
    "\n",
    "while not Resume_Training and MODEL_NAME==\"\":\n",
    "  print('No model found, use the \"Model Download\" cell to download a model.')\n",
    "  time.sleep(5)\n",
    "\n",
    "\n",
    "MODELT_NAME=MODEL_NAME\n",
    "untlr=UNet_Learning_Rate\n",
    "txlr=Text_Encoder_Learning_Rate\n",
    "\n",
    "trnonltxt=\"\"\n",
    "if UNet_Training_Steps==0:\n",
    "   trnonltxt=\"--train_only_text_encoder\"\n",
    "\n",
    "Seed='' \n",
    "\n",
    "Style=\"\"\n",
    "if Style_Training:\n",
    "  Style=\"--Style\"\n",
    "\n",
    "Res=int(Resolution)\n",
    "\n",
    "fp16 = True\n",
    "\n",
    "if Seed =='' or Seed=='0':\n",
    "  Seed=random.randint(1, 999999)\n",
    "else:\n",
    "  Seed=int(Seed)\n",
    "\n",
    "GC=\"--gradient_checkpointing\"\n",
    "\n",
    "if fp16:\n",
    "  prec=\"fp16\"\n",
    "else:\n",
    "  prec=\"no\"\n",
    "\n",
    "s = getoutput('nvidia-smi')\n",
    "if 'A100' in s:\n",
    "  GC=\"\"\n",
    "\n",
    "precision=prec\n",
    "\n",
    "resuming=\"\"\n",
    "if Resume_Training and os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n",
    "  MODELT_NAME=OUTPUT_DIR\n",
    "  print('Resuming Training...m')\n",
    "  resuming=\"Yes\"\n",
    "elif Resume_Training and not os.path.exists(OUTPUT_DIR+'/unet/diffusion_pytorch_model.bin'):\n",
    "  print('Previous model not found, training a new model...')\n",
    "  MODELT_NAME=MODEL_NAME\n",
    "  while MODEL_NAME==\"\":\n",
    "    print('No model found, use the \"Model Download\" cell to download a model.')\n",
    "    time.sleep(5)\n",
    "\n",
    "V2=False\n",
    "if os.path.getsize(MODELT_NAME+\"/text_encoder/pytorch_model.bin\") > 670901463:\n",
    "  V2=True\n",
    "\n",
    "Enable_text_encoder_training= True \n",
    "Enable_Text_Encoder_Concept_Training= True\n",
    "\n",
    "if Text_Encoder_Training_Steps==0:\n",
    "   Enable_text_encoder_training= False\n",
    "else:\n",
    "  stptxt=Text_Encoder_Training_Steps\n",
    "\n",
    "if Text_Encoder_Concept_Training_Steps==0:\n",
    "   Enable_Text_Encoder_Concept_Training= False\n",
    "else:\n",
    "  stptxtc=Text_Encoder_Concept_Training_Steps\n",
    "\n",
    "if Enable_text_encoder_training:\n",
    "  Textenc=\"--train_text_encoder\"\n",
    "else:\n",
    "  Textenc=\"\"\n",
    "\n",
    "if Save_Checkpoint_Every==None:\n",
    "  Save_Checkpoint_Every=1\n",
    "\n",
    "stp=0\n",
    "\n",
    "if Start_saving_from_the_step==None:\n",
    "  Start_saving_from_the_step=0\n",
    "if (Start_saving_from_the_step < 200):\n",
    "  Start_saving_from_the_step=Save_Checkpoint_Every\n",
    "stpsv=Start_saving_from_the_step\n",
    "if Save_Checkpoint_Every_n_Steps:\n",
    "  stp=Save_Checkpoint_Every\n",
    "\n",
    "def dump_only_textenc(trnonltxt, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, precision, Training_Steps):\n",
    "    !accelerate launch ./diffusers/examples/dreambooth/train_dreambooth.py \\\n",
    "    $trnonltxt \\\n",
    "    --image_captions_filename \\\n",
    "    --train_text_encoder \\\n",
    "    --dump_only_text_encoder \\\n",
    "    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n",
    "    --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
    "    --output_dir=\"$OUTPUT_DIR\" \\\n",
    "    --instance_prompt=\"$PT\" \\\n",
    "    --seed=$Seed \\\n",
    "    --resolution=512 \\\n",
    "    --mixed_precision=$precision \\\n",
    "    --train_batch_size=1 \\\n",
    "    --gradient_accumulation_steps=1 $GC \\\n",
    "    --use_8bit_adam \\\n",
    "    --learning_rate=$txlr \\\n",
    "    --lr_scheduler=\"polynomial\" \\\n",
    "    --lr_warmup_steps=0 \\\n",
    "    --max_train_steps=$Training_Steps\n",
    "\n",
    "def train_only_unet(stpsv, stp, SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps):\n",
    "    clear_output()\n",
    "    if resuming==\"Yes\":\n",
    "      print('Resuming Training...')    \n",
    "    print('Training the UNet...')\n",
    "    !accelerate launch ./diffusers/examples/dreambooth/train_dreambooth.py \\\n",
    "    $Style \\\n",
    "    --image_captions_filename \\\n",
    "    --train_only_unet \\\n",
    "    --save_starting_step=$stpsv \\\n",
    "    --save_n_steps=$stp \\\n",
    "    --Session_dir=$SESSION_DIR \\\n",
    "    --pretrained_model_name_or_path=\"$MODELT_NAME\" \\\n",
    "    --instance_data_dir=\"$INSTANCE_DIR\" \\\n",
    "    --output_dir=\"$OUTPUT_DIR\" \\\n",
    "    --instance_prompt=\"$PT\" \\\n",
    "    --seed=$Seed \\\n",
    "    --resolution=$Res \\\n",
    "    --mixed_precision=$precision \\\n",
    "    --train_batch_size=1 \\\n",
    "    --gradient_accumulation_steps=1 $GC \\\n",
    "    --use_8bit_adam \\\n",
    "    --learning_rate=$untlr \\\n",
    "    --lr_scheduler=\"polynomial\" \\\n",
    "    --lr_warmup_steps=0 \\\n",
    "    --max_train_steps=$Training_Steps\n",
    "\n",
    "if Enable_text_encoder_training :\n",
    "  print('Training the text encoder...')\n",
    "  if os.path.exists(OUTPUT_DIR+'/'+'text_encoder_trained'):\n",
    "    %rm -r $OUTPUT_DIR\"/text_encoder_trained\"\n",
    "  dump_only_textenc(trnonltxt, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, precision, Training_Steps=stptxt)\n",
    "\n",
    "if Enable_Text_Encoder_Concept_Training:\n",
    "  if os.path.exists(CONCEPT_DIR):\n",
    "    if os.listdir(CONCEPT_DIR)!=[]:\n",
    "      clear_output()\n",
    "      if resuming==\"Yes\":\n",
    "        print('Resuming Training...')    \n",
    "      print('Training the text encoder on the concept...')\n",
    "      dump_only_textenc(trnonltxt, MODELT_NAME, CONCEPT_DIR, OUTPUT_DIR, PT, Seed, precision, Training_Steps=stptxtc)\n",
    "    else:\n",
    "      clear_output()\n",
    "      if resuming==\"Yes\":\n",
    "        print('Resuming Training...')      \n",
    "      print('No concept images found, skipping concept training...')\n",
    "      time.sleep(8)\n",
    "  else:\n",
    "      clear_output()\n",
    "      if resuming==\"Yes\":\n",
    "        print('Resuming Training...')\n",
    "      print('No concept images found, skipping concept training...')\n",
    "      time.sleep(8)\n",
    "\n",
    "if UNet_Training_Steps!=0:\n",
    "  train_only_unet(stpsv, stp, SESSION_DIR, MODELT_NAME, INSTANCE_DIR, OUTPUT_DIR, PT, Seed, Res, precision, Training_Steps=UNet_Training_Steps)\n",
    "    \n",
    "\n",
    "if os.path.exists(INSTANCE_NAME+'/unet/diffusion_pytorch_model.bin'):\n",
    "  prc=\"--fp16\" if precision==\"fp16\" else \"\"\n",
    "  if V2:\n",
    "    !python ./diffusers/scripts/convertosdv2.py $prc $OUTPUT_DIR $SESSION_DIR/$Session_Name\".ckpt\"\n",
    "    clear_output()\n",
    "    if os.path.exists(SESSION_DIR+\"/\"+INSTANCE_NAME+'.ckpt'):\n",
    "      clear_output()\n",
    "      print(\"DONE, the CKPT model is in your drive in the sessions folder\")     \n",
    "    else:\n",
    "      print(\"Something went wrong\")     \n",
    "  else:  \n",
    "    clear_output()\n",
    "    if precision==\"no\":\n",
    "      !sed -i '226s@.*@@' ./convertosd.py\n",
    "    !sed -i '201s@.*@    model_path = \"{OUTPUT_DIR}\"@' ./convertosd.py\n",
    "    !sed -i '202s@.*@    checkpoint_path= \"{SESSION_DIR}/{Session_Name}.ckpt\"@' ./convertosd.py\n",
    "    !python convertosd.py\n",
    "    clear_output()\n",
    "    if os.path.exists(SESSION_DIR+\"/\"+INSTANCE_NAME+'.ckpt'):      \n",
    "      print(\"DONE, the CKPT model is in your drive in the sessions folder\")\n",
    "    else:\n",
    "      print(\"Something went wrong\")\n",
    "    \n",
    "else:\n",
    "  print(\"Something went wrong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (476867567.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0. warnings.warn(warning + message, FutureWarning\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_ddpm.DDPMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0. warnings.warn(warning + message, FutureWarning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9d05c6fb3a6b8c81130f36cf60ecdc9fce058fee2d66e936e2160c69499d4b0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
